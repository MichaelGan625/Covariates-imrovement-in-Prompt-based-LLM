D:\conda_envs\iz310\lib\site-packages\requests\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
D:\conda_envs\iz310\lib\site-packages\transformers\utils\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using a total of 150 function evaluations
Set all the seeds to 42 successfully!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|¨€¨€¨€¨€¨€     | 1/2 [00:10<00:10, 10.93s/it]Loading checkpoint shards: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 2/2 [00:19<00:00,  9.74s/it]Loading checkpoint shards: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 2/2 [00:19<00:00,  9.92s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
D:\Py\LLM test\run_instructzero.py:705: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
Shape of initial prompt embedding: torch.Size([1, 3, 4096])
Instruction: ['Input: 100\nOutput: 100\nInput: 24\nOutput: 24\nInput: 90\nOutput: 90\nInput: 117\nOutput: 117\nInput: 217\nOutput: 21']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_060938__em__Input_100_Output_100_Input_24_Output_24___e0b961c5.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_060946__em__Input_100_Output_fermium__2bb7265c.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\n\nInput: 24\nOutput: chromium\n\nInput: 90\nOutput: thorium\n\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganess']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_061039__em__Input_100_Output_fermium_Input_24_Output__e782bc95.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\n\nInput: 90\nOutput: thorium\n\nInput: 117\nOutput: tennessine\n\nInput: 118\nOutput: oganess']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_061149__em__Input_100_Output_fermium_Input_24_Output__9eb44241.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_061242__em__Input_100_Output_fermium_Input_24_Output__ccc293f6.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: ONE\nOutput: ONE']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_061601__em__Input_ONE_Output_ONE__71f7c9ab.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput:']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_061702__em__Input_100_Output_fermium_Input_24_Output__a48ef90c.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\n\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_061922__em__Input_100_Output_fermium_Input_24_Output__2293844e.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Instruction: ['Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Best initial point: 0.000
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.015}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Bayes iterations:   0%|          | 0/5 [00:00<?, ?it/s][06:20:06] INFO [Iteration 0] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[06:20:07] INFO [PROFILE] GP fit: 0.352s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[06:20:07] INFO Iter 0 best_value=0.00000 gp_loss=117.85723
[06:20:08] INFO [PROFILE] Acquisition: 0.812s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[06:20:17] INFO [PROFILE] LLM eval candidate: 9.717s
[06:20:17] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  20%|¨€¨€        | 1/5 [00:10<00:43, 10.89s/it][06:20:17] INFO [Iteration 1] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[06:20:18] INFO [PROFILE] GP fit: 0.333s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[06:20:18] INFO Iter 1 best_value=0.00000 gp_loss=117.85724
[06:20:18] INFO [PROFILE] Acquisition: 0.733s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[06:20:29] INFO [PROFILE] LLM eval candidate: 11.042s
[06:20:29] INFO Best value so far: 0.00000
Bayes iterations:  40%|¨€¨€¨€¨€      | 2/5 [00:23<00:34, 11.62s/it][06:20:29] INFO [Iteration 2] X_train torch.Size([26, 30]), y_train torch.Size([26, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
D:\Py\LLM test\run_instructzero.py:165: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444
  gp_model = _SingleTaskGP(X_train, y_train, covar_module=covar_module).to(device)
[06:20:30] INFO [PROFILE] GP fit: 0.418s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[06:20:30] INFO Iter 2 best_value=0.00000 gp_loss=91.44200
[06:20:30] INFO [PROFILE] Acquisition: 0.446s
[06:20:51] INFO [PROFILE] LLM eval candidate: 21.011s
[06:20:51] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  60%|¨€¨€¨€¨€¨€¨€    | 3/5 [00:44<00:32, 16.30s/it][06:20:51] INFO [Iteration 3] X_train torch.Size([26, 30]), y_train torch.Size([26, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
D:\Py\LLM test\run_instructzero.py:165: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444
  gp_model = _SingleTaskGP(X_train, y_train, covar_module=covar_module).to(device)
[06:20:52] INFO [PROFILE] GP fit: 0.167s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[06:20:52] INFO Iter 3 best_value=0.00000 gp_loss=91.44200
[06:20:52] INFO [PROFILE] Acquisition: 0.441s
[06:21:02] INFO [PROFILE] LLM eval candidate: 9.809s
[06:21:02] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  80%|¨€¨€¨€¨€¨€¨€¨€¨€  | 4/5 [00:55<00:13, 13.98s/it][06:21:02] INFO [Iteration 4] X_train torch.Size([26, 30]), y_train torch.Size([26, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
D:\Py\LLM test\run_instructzero.py:165: InputDataWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444
  gp_model = _SingleTaskGP(X_train, y_train, covar_module=covar_module).to(device)
[06:21:02] INFO [PROFILE] GP fit: 0.174s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[06:21:02] INFO Iter 4 best_value=0.00000 gp_loss=91.44200
[06:21:02] INFO [PROFILE] Acquisition: 0.438s
[06:21:06] INFO [PROFILE] LLM eval candidate: 3.809s
[06:21:06] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 5/5 [00:59<00:00, 10.54s/it]Bayes iterations: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 5/5 [00:59<00:00, 11.95s/it]
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.0185}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.022}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ['Write ONE concise instruction that maps Input to Output above.']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_062029__em__Write_ONE_concise_instruction_that_maps___9e00c91e.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
[kernel] covariate hooks failed (ignored): operands could not be broadcast together with shapes (25,12) (26,1) 
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
[WARNING][Iteration 2] custom kernel GP fit failed, falling back to baseline. Error: The size of tensor a (26) must match the size of tensor b (25) at non-singleton dimension 2
[baseline] X_train shape: torch.Size([26, 30]), dtype: torch.float32, device: cuda:0
[baseline] y_train shape: torch.Size([26, 1]), std: 0.0000e+00, mean: 0.0000e+00
Instruction: ["I'm sorry, but I'm not sure what you're asking. Could you please provide more context or clarify your question?"]
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_062051__em__I_m_sorry__but_I_m_not_sure_what_you_re___a71998d8.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
[kernel] covariate hooks failed (ignored): operands could not be broadcast together with shapes (25,12) (26,1) 
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
[WARNING][Iteration 3] custom kernel GP fit failed, falling back to baseline. Error: The size of tensor a (26) must match the size of tensor b (25) at non-singleton dimension 2
[baseline] X_train shape: torch.Size([26, 30]), dtype: torch.float32, device: cuda:0
[baseline] y_train shape: torch.Size([26, 1]), std: 0.0000e+00, mean: 0.0000e+00
Instruction: ['Input: 100\nOutput: fermium']
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
[kernel] covariate hooks failed (ignored): operands could not be broadcast together with shapes (25,12) (26,1) 
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
[WARNING][Iteration 4] custom kernel GP fit failed, falling back to baseline. Error: The size of tensor a (26) must match the size of tensor b (25) at non-singleton dimension 2
[baseline] X_train shape: torch.Size([26, 30]), dtype: torch.float32, device: cuda:0
[baseline] y_train shape: torch.Size([26, 1]), std: 0.0000e+00, mean: 0.0000e+00
Instruction: ['ONE']
Using metric "em" for task "periodic_elements"...
[PRED-DUMP] wrote 0 rows to logs/preds\periodic_elements_20251018_062106__em__ONE__bc21e648.tsv
Dev loss: 0.0. Dev perf: 0.0. Best dev perf: 0.0
********* Done *********
Evaluate on test data...
Best instruction is:
['ONE']
The final instruction set is:
{'Input: 100\nOutput: 100\nInput: 24\nOutput: 24\nInput: 90\nOutput: 90\nInput: 117\nOutput: 117\nInput: 217\nOutput: 21': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: 100\nOutput: fermium': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: 100\nOutput: fermium\n\n\nInput: 24\nOutput: chromium\n\nInput: 90\nOutput: thorium\n\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganess': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\n\nInput: 90\nOutput: thorium\n\nInput: 117\nOutput: tennessine\n\nInput: 118\nOutput: oganess': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: ONE\nOutput: ONE': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: 100\nOutput: fermium\nInput: 24\nOutput: chromium\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson\nInput:': (0.0, array([], shape=(1, 0), dtype=float64)), 'Input: 100\nOutput: fermium\n\nInput: 24\nOutput: chromium\n\nInput: 90\nOutput: thorium\nInput: 117\nOutput: tennessine\nInput: 118\nOutput: oganesson': (0.0, array([], shape=(1, 0), dtype=float64)), 'Write ONE concise instruction that maps Input to Output above.': (0.0, array([], shape=(1, 0), dtype=float64)), "I'm sorry, but I'm not sure what you're asking. Could you please provide more context or clarify your question?": (0.0, array([], shape=(1, 0), dtype=float64)), 'ONE': (0.0, array([], shape=(1, 0), dtype=float64))}
Evaluating on test data...
Evaluating prompts...
Using metric "em" for task "periodic_elements"...
[PRED 0] gold=['tungsten'] | pred='seventy-four' | score=0.0000
[PRED 1] gold=['copernicium'] | pred='one' | score=0.0000
[PRED 2] gold=['arsenic'] | pred='thirty-three' | score=0.0000
[PRED 3] gold=['radon'] | pred='eighty-six' | score=0.0000
[PRED 4] gold=['hassium'] | pred='multiply the input number by 1.\n\noutput: 108' | score=0.0000
[PRED-DUMP] wrote 50 rows to logs/preds\periodic_elements_20251018_062148__em__ONE__bc21e648.tsv
Finished evaluating.
Finished!!!
Test score on ChatGPT: 0.0
