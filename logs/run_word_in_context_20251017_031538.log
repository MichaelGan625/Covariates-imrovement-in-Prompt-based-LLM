D:\conda_envs\iz310\lib\site-packages\requests\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
D:\conda_envs\iz310\lib\site-packages\transformers\utils\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Using a total of 150 function evaluations
Set all the seeds to 42 successfully!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|¨€¨€¨€¨€¨€     | 1/2 [00:11<00:11, 11.70s/it]Loading checkpoint shards: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 2/2 [00:20<00:00,  9.95s/it]Loading checkpoint shards: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 2/2 [00:20<00:00, 10.21s/it]
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
D:\Py\LLM test\run_instructzero.py:705: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
Shape of initial prompt embedding: torch.Size([1, 3, 4096])
Instruction: ['Input: Sentence 1: The first letter of the alphabet. Sentence 2: The second letter of the alphabet. Word: alphabet. Output: not the same\nInput: Sentence 1: The first word of the sentence. Sentence 2: The second word of the sentence. Word:']
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same"]
Instruction: ['Input: Draw on a map. Sentence 2: A map is not your friend. Word: map\nOutput: not the same\nInput: His instructions were clear and concise. Sentence 2: A clear conscience is a good thing. Word: clear\nOutput: same\nInput: The ball']
Instruction: ["Input:\n\n\n1. Move in a matter.\n2. Come on guys, let's move: there's work to do!\n\n\nOutput:\n\n1. Same\n2. Not the same\n\n\nInput:\n3. Draw on a cigarette.\n4"]
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same\nInput: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire."]
Instruction: ["Input: Sentence 1: Move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: move"]
Instruction: ["Input: Move in a matter.\nOutput: Same\n\nInput: Come on guys, let's move: there's work to do!\nOutput: Same\nInput: Word: move\nOutput: Same\n\nInput: Sentence 1: Draw on a cigarette.\nOutput:"]
Instruction: ['Input: Move in a matter.\n\nOutput: Same.\n\nInput: Draw on a cigarette.\nOutput: Not the same.\n\nInput: His instructions deliberately gave them the wrong set.\nOutput: Not the same.\n\nInput: The ball travelled 90 m']
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same\n\nInput: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire"]
Instruction: ["Input:\n\n* Move in a matter.\n* Come on guys, let's move: there's work to do!\n* Draw on a cigarette.\n* Draw wire.\n* Move\n\nOutput:\n* Same\n\nInput:\n* Draw on a cigarette"]
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: not the same\nInput: Sentence 1: The ball travelled 90 mph on his serve."]
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same"]
Instruction: ['Input: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire. Word: draw\nOutput: not the same\n\nInput: Sentence 1: The ball travelled 90 mph on his serve. Sentence 2: The mayor threw out the first']
Instruction: ['Input: Move in a matter.\nOutput: Same.']
Instruction: ["Input: Sentence 1: Move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same\n\nInput: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire."]
Instruction: ['Input: Move in a matter.\nOutput: same\n\nInput: Draw on a cigarette.\nOutput: not the same\nInput: The ball travelled 90 mph on his serve.\nOutput: same\nInput: We carry a very large mortgage.\nOutput: same']
Instruction: ["Input: Move in a matter.\nOutput: Same.\n\nInput: Come on guys, let's move: there's work to do!\nOutput: Same.\n\nInput: Draw on a cigarette.\nOutput: Not the same.\n\nInput: The ball travelled"]
Instruction: ['Input: Move in a matter.\nOutput: Same.']
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same\n\nInput: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire"]
Instruction: ["Input: Sentence 1: To move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same\nInput: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire."]
Instruction: ["Input:\n\n\nSentence 1: To move in a matter.\nSentence 2: Come on guys, let's move: there's work to do!\nWord: move\n\n\nOutput: same\n\n\nSentence 1: Draw on a cigaret"]
Instruction: ['Input: Move in a matter.\nOutput: Same.']
Instruction: ['Input: Draw on a cigarette. Sentence 2: Draw wire. Word: draw\nOutput: not the same\n\nInput: His instructions deliberately gave them the wrong set. Sentence 2: A set of tools. Word: set\nOutput: not the same\nInput: The ball']
Instruction: ['Input: Move in a matter.\nOutput: Same.']
Instruction: ["Input: Sentence 1: Move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same\n\nInput: Sentence 1: Draw on a cigarette. Sentence 2: Draw wire."]
Best initial point: 0.000
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.015}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Bayes iterations:   0%|          | 0/5 [00:00<?, ?it/s][03:35:12] INFO [Iteration 0] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[03:35:12] INFO [PROFILE] GP fit: 0.337s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[03:35:12] INFO Iter 0 best_value=0.00000 gp_loss=117.85723
[03:35:13] INFO [PROFILE] Acquisition: 0.846s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[03:35:49] INFO [PROFILE] LLM eval candidate: 36.542s
[03:35:49] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  20%|¨€¨€        | 1/5 [00:37<02:30, 37.74s/it][03:35:49] INFO [Iteration 1] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[03:35:50] INFO [PROFILE] GP fit: 0.343s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[03:35:50] INFO Iter 1 best_value=0.00000 gp_loss=117.85724
[03:35:50] INFO [PROFILE] Acquisition: 0.735s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[03:36:06] INFO [PROFILE] LLM eval candidate: 16.075s
[03:36:06] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  40%|¨€¨€¨€¨€      | 2/5 [00:54<01:16, 25.64s/it][03:36:06] INFO [Iteration 2] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[03:36:07] INFO [PROFILE] GP fit: 0.362s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[03:36:07] INFO Iter 2 best_value=0.00000 gp_loss=117.85726
[03:36:08] INFO [PROFILE] Acquisition: 0.746s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[03:36:15] INFO [PROFILE] LLM eval candidate: 7.614s
[03:36:15] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  60%|¨€¨€¨€¨€¨€¨€    | 3/5 [01:03<00:35, 17.92s/it][03:36:15] INFO [Iteration 3] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[03:36:16] INFO [PROFILE] GP fit: 0.360s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[03:36:16] INFO Iter 3 best_value=0.00000 gp_loss=117.85727
[03:36:16] INFO [PROFILE] Acquisition: 0.737s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[03:36:24] INFO [PROFILE] LLM eval candidate: 7.729s
[03:36:24] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations:  80%|¨€¨€¨€¨€¨€¨€¨€¨€  | 4/5 [01:12<00:14, 14.33s/it][03:36:24] INFO [Iteration 4] X_train torch.Size([25, 30]), y_train torch.Size([25, 1])
D:\Py\LLM test\run_instructzero.py:780: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\aten\src\ATen\native\ReduceOps.cpp:1839.)
  col_std = S.std(dim=0, keepdim=True).clamp_min(1e-3)
[03:36:24] INFO [PROFILE] GP fit: 0.344s
D:\conda_envs\iz310\lib\site-packages\gpytorch\models\exact_gp.py:296: GPInputWarning: The input matches the stored training data. Did you forget to call model.train()?
  warnings.warn(
[03:36:24] INFO Iter 4 best_value=0.00000 gp_loss=117.85727
[03:36:25] INFO [PROFILE] Acquisition: 0.733s
D:\conda_envs\iz310\lib\site-packages\gpytorch\distributions\multivariate_normal.py:376: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-10.
  warnings.warn(
[03:36:33] INFO [PROFILE] LLM eval candidate: 7.493s
[03:36:33] INFO Invalid/fallback instruction; skip appending to training set.
Bayes iterations: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 5/5 [01:21<00:00, 12.26s/it]Bayes iterations: 100%|¨€¨€¨€¨€¨€¨€¨€¨€¨€¨€| 5/5 [01:21<00:00, 16.21s/it]
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.0185}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ["Input: Sentence 1: Move in a matter. Sentence 2: Come on guys, let's move: there's work to do! Word: move\nOutput: same"]
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.022}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ['Sentence 1: The instruction is clear. Output: same']
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.025500000000000002}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ['ONE']
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.028999999999999998}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ['ONE']
[kernel] original_dominant {'alpha_lat': 0.05, 'alpha_instr': 0.05, 'alpha_cov': 0.032499999999999994}
Custom kernel sample K shape: torch.Size([3, 3]) has NaN: False
Instruction: ['ONE']
Evaluate on test data...
Best instruction is:
None
The final instruction set is:
{}
Evaluating on test data...
Evaluating prompts...
Traceback (most recent call last):
  File "D:\Py\LLM test\run_instructzero.py", line 1054, in <module>
    test_score = run(args=args)
  File "D:\Py\LLM test\run_instructzero.py", line 1036, in run
    test_res = ape.evaluate_prompts(
  File "D:\Py\LLM test\automatic_prompt_engineer\ape.py", line 185, in evaluate_prompts
    res = evaluate.evaluate_prompts(
  File "D:\Py\LLM test\automatic_prompt_engineer\evaluate.py", line 44, in evaluate_prompts
    return eval_method(prompts, eval_template, eval_data, demos_template, few_shot_data, config)
  File "D:\Py\LLM test\evaluation_instruction_induction\exec_accuracy.py", line 162, in exec_accuracy_evaluator
    for prompt in prompts:
TypeError: 'NoneType' object is not iterable
