import torch
import random
import numpy as np
from evaluation_instruction_induction.exec_accuracy import exec_accuracy_evaluator
import os

TASKS = [
    'antonyms', 
    'cause_and_effect', 
    'common_concept', 
    'diff', 
    'first_word_letter', 
    'informal_to_formal', 
    'larger_animal', 
    'letters_list', 
    'taxonomy_animal', 
    'negation', 
    'num_to_verbal', 
    'active_to_passive', 
    'singular_to_plural', 
    'rhymes', 
    'second_word_letter', 
    'sentence_similarity', 
    'sentiment', 
    'orthography_starts_with', 
    'sum', 
    'synonyms', 
    'translation_en-de', 
    'translation_en-es', 
    'translation_en-fr', 
    'word_in_context', 
    'auto_categorization', 
    'auto_debugging', 
    'ascii', 
    'cs_algorithms', 
    'periodic_elements', 
    'word_sorting', 
    'word_unscrambling', 
    'odd_one_out', 
    'object_count'
]

SMOKE_TEST = os.environ.get("SMOKE_TEST")
## bayesian opt
tkwargs = {
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu"),
    "dtype": torch.float64,  # ğŸŸ¢ [ä¿®æ”¹] å¼ºçƒˆå»ºè®®æ”¹ä¸º float64ï¼Œä»¥è·å¾—æ›´é«˜çš„æ•°å€¼ç²¾åº¦
}

N_INIT = 35
N_ITERATIONS = 40 if not SMOKE_TEST else 1
BATCH_SIZE = 10 if not SMOKE_TEST else 1


def get_test_conf(task, test_data):
    test_conf = {
        'generation': {
            'num_subsamples': 3,
            'num_demos': 5,
            'num_prompts_per_subsample': 0,
            'model': {
                'gpt_config': {
                    'model': 'meta-llama/llama-3-8b-instruct',  # ä¿®æ”¹è¿™é‡Œ
                    'api_base': 'https://openrouter.ai/api/v1',
                }
            }
        },
        'evaluation': {
            'method': exec_accuracy_evaluator,
            'num_samples': min(100, len(test_data[0])),
            'task': task,
            'model': {
                "name": "GPT_forward",
                'gpt_config': {
                    'model': 'meta-llama/llama-3-8b-instruct',  # ä¿®æ”¹è¿™é‡Œ
                    'api_base': 'https://openrouter.ai/api/v1',   
                }
            }
        }
    }
    return test_conf
def get_conf(task, eval_data):
    conf = {
        'generation': {
            'num_subsamples': 1,
            'num_demos': 10,
            'num_prompts_per_subsample': 20,
            'model': {
                'name': 'GPT_forward',  # <--- å¿…é¡»æ·»åŠ è¿™ä¸€è¡Œ
                'gpt_config': {
                    'model': 'meta-llama/llama-3-8b-instruct',
                    'api_base': 'https://openrouter.ai/api/v1',
                }
            }
        },
        'evaluation': {
            'method': exec_accuracy_evaluator,
            'task': task,
            'num_samples': min(30, len(eval_data[0])),
            'model': {
                'name': 'GPT_forward',  # <--- å¿…é¡»æ·»åŠ è¿™ä¸€è¡Œ
                'gpt_config': {
                    'model': 'meta-llama/llama-3-8b-instruct',
                    'api_base': 'https://openrouter.ai/api/v1',
                }
            }
        }
    }
    return conf

def set_all_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    return f"Set all the seeds to {seed} successfully!"


# ==== misc.py è¿½åŠ ï¼šå››åå˜é‡ç‰¹å¾ ====
import re
import numpy as np
from typing import List, Optional

_REFUSAL_KEYS = ["i'm sorry", "cannot", "ä¸èƒ½", "æ— æ³•", "ä½œä¸º", "ä½œä¸ºä¸€ä¸ª"]
_CONSTRAINT_KEYS = ["only", "exactly", "do not", "without", "åª", "ä¸å¾—", "å¿…é¡»", "ä»…è¾“å‡º", "åªè¾“å‡º", "ä¸€è¯",
                    "single word"]


def _bow_vec(s: str):
    v = {}
    for w in re.findall(r"\w+", (s or "").lower()):
        v[w] = v.get(w, 0) + 1
    return v


def cosine_bow(a: str, b: str) -> float:
    va, vb = _bow_vec(a), _bow_vec(b)
    inter = set(va) & set(vb)
    dot = sum(va[w] * vb[w] for w in inter)
    na = sum(x * x for x in va.values()) ** 0.5
    nb = sum(x * x for x in vb.values()) ** 0.5
    return 0.0 if na == 0 or nb == 0 else float(dot / (na * nb))


def estimate_instruction_perplexity(prompt_text: str, scorer=None) -> float:
    """
    ä¼°è®¡æŒ‡ä»¤å›°æƒ‘åº¦ï¼š
    - è‹¥ä¼ å…¥ scorer ä¸”æœ‰ score_texts(texts)->neg_loglik/token_count æ¥å£ï¼Œåˆ™ç”¨ä¹‹ï¼›
    - å¦åˆ™ç”¨â€œé•¿åº¦ä»£ç†â€ï¼ˆlog(len+1)ï¼‰å…œåº•ï¼Œé¿å…é¢å¤–ä¾èµ–ã€‚
    """
    if scorer is not None:
        try:
            # æœŸæœ›ï¼šè¿”å›å¹³å‡è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆæˆ– pplï¼‰ï¼Œä½ å¯åœ¨æœ¬åœ° LLaMA/T5 ä¸Šå®ç°
            return float(scorer(prompt_text))
        except Exception:
            pass
    # å…œåº•ï¼šé•¿åº¦ä»£ç†ï¼ˆè¶ŠçŸ­è¶Šâ€œæ˜“è¯»â€ï¼Œç»™ä¸ªå•è°ƒæ›¿ä»£ï¼‰
    return float(np.log(len(prompt_text.strip()) + 1.0))


def predict_compliance_from_prompt(task: str, prompt_text: str) -> float:
    """
    åœ¨æ²¡æœ‰æ¨¡å‹è¾“å‡ºæ—¶ï¼ŒåŸºäºæç¤ºä¸­çš„çº¦æŸå¼ºåº¦ç²—ç•¥é¢„æµ‹â€œåˆè§„ç‡â€ã€‚
    å¼ºçº¦æŸè¯è¶Šå¤šã€æ‹’ç»è¯è¶Šå°‘ã€è¶Šç®€çŸ­ => é¢„æµ‹åˆè§„ç‡è¶Šé«˜ã€‚
    """
    p = (prompt_text or "").lower()
    cons = sum(k in p for k in _CONSTRAINT_KEYS)
    refus = sum(k in p for k in _REFUSAL_KEYS)
    L = max(1, len(prompt_text.strip()))
    score = cons * 1.0 - refus * 1.5 - 0.002 * L  # çº¿æ€§æ‰“åˆ†
    # æ˜ å°„åˆ° (0,1)
    return float(1.0 / (1.0 + np.exp(-score)))


def compute_compliance_rate_from_preds(task: str, preds: List[str]) -> float:
    """
    ç”¨çœŸå®æ¨¡å‹é¢„æµ‹è¾“å‡ºè®¡ç®—â€œåˆè§„ç‡â€ï¼šè¿™é‡Œç»™é€šç”¨è§„åˆ™ï¼š
    - å•è¯ä»»åŠ¡ï¼ˆantonyms/synonyms/taxonomy_animal/orthography_starts_with ç­‰ï¼‰ï¼šå¿…é¡»æ˜¯å•ä¸ªè¯ã€æ— å¼•å·ã€æ— æ˜æ˜¾æ ‡ç‚¹
    - å…¶ä»–ï¼šåªæ£€æŸ¥â€œä¸è¦è§£é‡Šæ€§å‰ç¼€â€å’Œâ€œæ— æ‹’ç»è¯â€
    è‹¥ä½ æœ‰æ›´ç»†çš„ä»»åŠ¡æ­£åˆ™ï¼Œå¯åœ¨è¿™é‡Œç²¾ç‚¼ã€‚
    """
    if not preds:
        return 0.0
    single_word_like = {"antonyms", "synonyms", "taxonomy_animal", "orthography_starts_with"}
    punc = set(".,;:!?ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ'\"`")
    cnt = 0
    for t in preds:
        s = (t or "").strip()
        s_low = s.lower()
        if any(k in s_low for k in _REFUSAL_KEYS):
            continue
        if task in single_word_like:
            tokens = s.split()
            if len(tokens) != 1:
                continue
            if any(c in punc for c in s):
                continue
        # å¯ä»¥ç»§ç»­å åŠ ä½ ä»»åŠ¡ç‰¹æœ‰è§„åˆ™
        cnt += 1
    return float(cnt / len(preds))


def compute_four_covariates(
        prompt_text: str,
        task: str,
        preds: Optional[List[str]],
        answers: Optional[List[str]],
        S: Optional[np.ndarray],
        best_prompt_text: Optional[str],
        best_S: Optional[np.ndarray],
        ppl_scorer=None,
) -> np.ndarray:
    """
    4 ç»´ç‰¹å¾ï¼š [ sim_to_best, delta_best, -ppl, compliance ]
    - sim_to_bestï¼šä¸å†å²å½“å‰æœ€ä¼˜æŒ‡ä»¤çš„ BoW ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ— æœ€ä¼˜åˆ™ 0ï¼‰
    - delta_bestï¼šmean(S - best_S)ï¼Œæ—  S/best_S åˆ™ 0
    - -ppl    ï¼šæŒ‡ä»¤å›°æƒ‘åº¦å–è´Ÿå·ï¼ˆå›°æƒ‘åº¦è¶Šä½è¶Šå¥½ => ç‰¹å¾è¶Šå¤§ï¼‰
    - complianceï¼šæœ‰ preds å°±ç”¨çœŸå®åˆè§„ç‡ï¼Œå¦åˆ™ç”¨ prompt ç»“æ„é¢„æµ‹
    """
    sim_to_best = cosine_bow(prompt_text, best_prompt_text or "")
    if S is not None and best_S is not None and len(S) == len(best_S):
        delta_best = float(np.mean(S - best_S))
        delta_best = max(delta_best, 0.0)  # åªå¼ºè°ƒâ€œæå‡è¾¹é™…â€
    else:
        delta_best = 0.0
    ppl = estimate_instruction_perplexity(prompt_text, scorer=ppl_scorer)
    neg_ppl = -float(ppl)
    if preds is not None:
        compliance = compute_compliance_rate_from_preds(task, preds)
    else:
        compliance = predict_compliance_from_prompt(task, prompt_text)
    return np.array([sim_to_best, delta_best, neg_ppl, compliance], dtype=np.float32)
